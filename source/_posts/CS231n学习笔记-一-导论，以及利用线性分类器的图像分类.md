---
layout: post
title: 'CS231n学习笔记(一): 导论，以及利用线性分类器的图像分类'
date: 2025-08-05 01:22:23
categories:
  - 计算机
tags:
  - 学习笔记
  - AI
---
## KNN

K-近邻算法是一个经典的ML算法，并不适合用在图像处理中，主要原因是：

1. 训练快(只需要将训练集保存起来, $O(N)$ ), 但推理慢(需要计算输入和每一个训练数据的距离 $O(M\times N)$ )。
2. 不管用什么距离算法，都不能很好的衡量两个图片的差别。

## 习题解析

实现L2距离计算:

```python
dists[i, j] = np.sqrt(((X[i] - self.X_train[j]) ** 2).sum())
```

将测试集和训练集中所有图片分别计算L2距离，绘制如下图：
![](./KNN_L2_Dist.png)
上图中白色表示距离远，黑色表示距离近。习题要求解释出现明亮行和列的原因。不难想到，每一行表示一个测试例，每一列表示一个训练例。因此，导致出现明亮行的数据，是一个与大部分测试例L2距离都很远的训练例；同理，导致出现明亮列的数据，是一个与大部分训练例L2距离都很远的测试例。
接下来实现`predict_labels`方法，首先要求实现我们获取前k近的标签，作业的建议是使用`np.argsort`，但最好使用`np.argpartition`:

```python
closest_y = self.y_train[np.argpartition(dists[i, :], k - 1)[:k]]
```

然后获取前k近的标签中出现次数最多的标签，并作为预测结果；如果有出现次数相同的标签，则选择最小的标签：

```python
val, count = np.unique(closest_y, return_counts=True)
y_pred[i] = val[count==np.max(count)][0]
```

这一段实现有点tricky，实际上是利用了`np.unique`函数不仅会返回所有元素出现的次数，还会自动排序元素的性质。这样我们只要取`val[count==np.max(count)]`的首元素，就自然能解决有多个出现次数相同的元素的场景。  
当`k=1`时，正确率为$27.4\\%$，当`k=5`时，正确率为$27.8\\%$。考虑到我们一共只有7类，目前KNN分类器的准确率只比$\frac{1}{7}$高$10\\%$  
